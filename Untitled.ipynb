{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "config = tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "from tensorflow.keras import  models, optimizers, layers, activations\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, RNN, GRU, Dense, Embedding\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set of hyperparameters to be tuned during the sweep\n",
    "\n",
    "default_parameters = dict(\n",
    "    embedding_size = 32,\n",
    "    batch_size = 32,\n",
    "    num_enc_layers = 3,\n",
    "    num_dec_layers = 3,\n",
    "    hidden_layer_size = 64,\n",
    "    cell_type = 'LSTM',\n",
    "    dropout = 0.2,\n",
    "    recurrent_dropout = 0.2,\n",
    "    epochs = 10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: W&B API key is configured (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.23<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">feasible-haze-9</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/arneshbose1/CS6910_Assignment3\" target=\"_blank\">https://wandb.ai/arneshbose1/CS6910_Assignment3</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/arneshbose1/CS6910_Assignment3/runs/18j90oj8\" target=\"_blank\">https://wandb.ai/arneshbose1/CS6910_Assignment3/runs/18j90oj8</a><br/>\n",
       "                Run data is saved locally in <code>E:\\CS6910 Assignments\\Assignment 3\\wandb\\run-20210428_101705-18j90oj8</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wandb login \n",
    "run = wandb.init(config=default_parameters, project=\"CS6910_Assignment3\", entity=\"arneshbose1\")\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the train, validation and test dataset\n",
    "\n",
    "train_path = 'dakshina_dataset_v1.0\\hi\\lexicons\\hi.translit.sampled.train.tsv'\n",
    "val_path = 'dakshina_dataset_v1.0\\hi\\lexicons\\hi.translit.sampled.dev.tsv'\n",
    "test_path = 'dakshina_dataset_v1.0\\hi\\lexicons\\hi.translit.sampled.test.tsv'\n",
    "\n",
    "# creating the corpus and vectorizing the data\n",
    "\n",
    "train_X = []\n",
    "train_Y = []\n",
    "input_corpus = set()\n",
    "output_corpus = set()\n",
    "\n",
    "with open(train_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "    \n",
    "for line in lines[:len(lines) - 1]:\n",
    "    target_text, input_text, _ = line.split(\"\\t\")\n",
    "    #using \"tab\" as the \"start sequence\" character for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = \"\\t\" + target_text + \"\\n\"\n",
    "    train_X.append(input_text)\n",
    "    train_Y.append(target_text)\n",
    "    for char in input_text:\n",
    "        input_corpus.add(char)\n",
    "    for char in target_text:\n",
    "        output_corpus.add(char)\n",
    "\n",
    "# ' ' is used to fill the empty spaces of shorter sequences\n",
    "input_corpus.add(\" \")\n",
    "output_corpus.add(\" \")\n",
    "input_corpus = sorted(list(input_corpus))\n",
    "output_corpus = sorted(list(output_corpus))\n",
    "num_encoder_tokens = len(input_corpus)\n",
    "num_decoder_tokens = len(output_corpus)\n",
    "max_encoder_seq_length = max([len(txt) for txt in train_X])\n",
    "max_decoder_seq_length = max([len(txt) for txt in train_Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X = []\n",
    "val_Y = []\n",
    "with open(val_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "    \n",
    "for line in lines[:len(lines) - 1]:\n",
    "    target_text, input_text, _ = line.split(\"\\t\")\n",
    "    target_text = \"\\t\" + target_text + \"\\n\"\n",
    "    val_X.append(input_text)\n",
    "    val_Y.append(target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 44204\n",
      "Number of unique input tokens: 27\n",
      "Number of unique output tokens: 66\n",
      "Max sequence length for inputs: 20\n",
      "Max sequence length for outputs: 21\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of samples:\", len(train_X))\n",
    "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
    "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
    "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
    "print(\"Max sequence length for outputs:\", max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_char_index = dict([(char, i) for i, char in enumerate(input_corpus)])\n",
    "output_char_index = dict([(char, i) for i, char in enumerate(output_corpus)])\n",
    "\n",
    "encoder_input_data = np.zeros((len(train_X), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\")\n",
    "decoder_input_data = np.zeros((len(train_X), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
    "decoder_target_data = np.zeros((len(train_X), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
    "\n",
    "for i, (x, y) in enumerate(zip(train_X, train_Y)):\n",
    "    for t, char in enumerate(x):\n",
    "        encoder_input_data[i, t, input_char_index[char]] = 1.0\n",
    "        \n",
    "    encoder_input_data[i, t + 1 :, input_char_index[\" \"]] = 1.0\n",
    "    \n",
    "    for t, char in enumerate(y):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, output_char_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, output_char_index[char]] = 1.0\n",
    "            \n",
    "    decoder_input_data[i, t + 1 :, output_char_index[\" \"]] = 1.0\n",
    "    decoder_target_data[i, t:, output_char_index[\" \"]] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data_val = np.zeros((len(val_X), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\")\n",
    "decoder_input_data_val = np.zeros((len(val_X), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
    "decoder_target_data_val = np.zeros((len(val_X), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
    "\n",
    "for i, (x, y) in enumerate(zip(val_X, val_Y)):\n",
    "    for t, char in enumerate(x):\n",
    "        encoder_input_data_val[i, t, input_char_index[char]] = 1.0\n",
    "        \n",
    "    encoder_input_data_val[i, t + 1 :, input_char_index[\" \"]] = 1.0\n",
    "    \n",
    "    for t, char in enumerate(y):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data_val[i, t, output_char_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            decoder_target_data_val[i, t - 1, output_char_index[char]] = 1.0\n",
    "            \n",
    "    decoder_input_data_val[i, t + 1 :, output_char_index[\" \"]] = 1.0\n",
    "    decoder_target_data_val[i, t:, output_char_index[\" \"]] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = config.embedding_size\n",
    "batch_size = config.batch_size\n",
    "num_enc_layers = config.num_enc_layers\n",
    "num_dec_layers = config.num_dec_layers\n",
    "hidden_layer_size = config.hidden_layer_size\n",
    "cell_type = config.cell_type\n",
    "dropout = config.dropout\n",
    "recurrent_dropout = config.recurrent_dropout\n",
    "epochs = config.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(hidden_layer_size, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(hidden_layer_size, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation=\"softmax\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1382/1382 [==============================] - 12s 8ms/step - loss: 1.0161 - accuracy: 0.7402 - val_loss: 0.7870 - val_accuracy: 0.7804\n",
      "Epoch 2/10\n",
      "1382/1382 [==============================] - 10s 7ms/step - loss: 0.7060 - accuracy: 0.7991 - val_loss: 0.5873 - val_accuracy: 0.8269\n",
      "Epoch 3/10\n",
      "1382/1382 [==============================] - 11s 8ms/step - loss: 0.5430 - accuracy: 0.8384 - val_loss: 0.4721 - val_accuracy: 0.8593\n",
      "Epoch 4/10\n",
      "1382/1382 [==============================] - 10s 8ms/step - loss: 0.4437 - accuracy: 0.8649 - val_loss: 0.4033 - val_accuracy: 0.8761\n",
      "Epoch 5/10\n",
      "1382/1382 [==============================] - 13s 9ms/step - loss: 0.3808 - accuracy: 0.8828 - val_loss: 0.3534 - val_accuracy: 0.8902\n",
      "Epoch 6/10\n",
      "1382/1382 [==============================] - 10s 8ms/step - loss: 0.3367 - accuracy: 0.8951 - val_loss: 0.3280 - val_accuracy: 0.8997\n",
      "Epoch 7/10\n",
      "1382/1382 [==============================] - 11s 8ms/step - loss: 0.3057 - accuracy: 0.9043 - val_loss: 0.2981 - val_accuracy: 0.9069\n",
      "Epoch 8/10\n",
      "1382/1382 [==============================] - 11s 8ms/step - loss: 0.2821 - accuracy: 0.9114 - val_loss: 0.2937 - val_accuracy: 0.9065\n",
      "Epoch 9/10\n",
      "1382/1382 [==============================] - 11s 8ms/step - loss: 0.2641 - accuracy: 0.9169 - val_loss: 0.2763 - val_accuracy: 0.9141\n",
      "Epoch 10/10\n",
      "1382/1382 [==============================] - 12s 8ms/step - loss: 0.2496 - accuracy: 0.9214 - val_loss: 0.2567 - val_accuracy: 0.9207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b52a480520>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit([encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=([encoder_input_data_val, decoder_input_data_val],decoder_target_data_val),\n",
    "    callbacks=[WandbCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = model.input[0]  # input_1\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_inputs = model.input[1]  # input_2\n",
    "decoder_state_input_h = Input(shape=(hidden_layer_size,), name=\"input_3\")\n",
    "decoder_state_input_c = Input(shape=(hidden_layer_size,), name=\"input_5\")\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_lstm = model.layers[3]\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "decoder_dense = model.layers[4]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
    ")\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_char_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in output_char_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, output_char_index[\"\\t\"]] = 1.0\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: an\n",
      "Decoded sentence: आन\n",
      "\n",
      "-\n",
      "Input sentence: ankganit\n",
      "Decoded sentence: अंगकनी\n",
      "\n",
      "-\n",
      "Input sentence: uncle\n",
      "Decoded sentence: एनक्ल\n",
      "\n",
      "-\n",
      "Input sentence: ankur\n",
      "Decoded sentence: अनकूर\n",
      "\n",
      "-\n",
      "Input sentence: ankuran\n",
      "Decoded sentence: अनुकरान\n",
      "\n",
      "-\n",
      "Input sentence: ankurit\n",
      "Decoded sentence: अनकुरित\n",
      "\n",
      "-\n",
      "Input sentence: aankush\n",
      "Decoded sentence: अनकूष\n",
      "\n",
      "-\n",
      "Input sentence: ankush\n",
      "Decoded sentence: अंकुष\n",
      "\n",
      "-\n",
      "Input sentence: ang\n",
      "Decoded sentence: आंग\n",
      "\n",
      "-\n",
      "Input sentence: anga\n",
      "Decoded sentence: अंगा\n",
      "\n",
      "-\n",
      "Input sentence: agandh\n",
      "Decoded sentence: अग्धन\n",
      "\n",
      "-\n",
      "Input sentence: angad\n",
      "Decoded sentence: अंगड\n",
      "\n",
      "-\n",
      "Input sentence: angane\n",
      "Decoded sentence: अंगने\n",
      "\n",
      "-\n",
      "Input sentence: angbhang\n",
      "Decoded sentence: अंगभांग\n",
      "\n",
      "-\n",
      "Input sentence: angarakshak\n",
      "Decoded sentence: अंगरकाषक\n",
      "\n",
      "-\n",
      "Input sentence: angrakshak\n",
      "Decoded sentence: अंगरकाषक\n",
      "\n",
      "-\n",
      "Input sentence: angara\n",
      "Decoded sentence: अंगरा\n",
      "\n",
      "-\n",
      "Input sentence: angaare\n",
      "Decoded sentence: अंगारे\n",
      "\n",
      "-\n",
      "Input sentence: angare\n",
      "Decoded sentence: अंगरे\n",
      "\n",
      "-\n",
      "Input sentence: angi\n",
      "Decoded sentence: आंगी\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(20):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(\"-\")\n",
    "    print(\"Input sentence:\", train_X[seq_index])\n",
    "    print(\"Decoded sentence:\", decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
